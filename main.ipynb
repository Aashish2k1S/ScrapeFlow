{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe75a5a",
   "metadata": {},
   "source": [
    "1. üîß Installations (if running in Colab or Jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a889c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. üîß Installations (if running in Colab or Jupyter)\n",
    "!pip install dspy-ai pydantic beautifulsoup4 requests numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819782cd",
   "metadata": {},
   "source": [
    "2. üì• Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "import dspy\n",
    "from dspy.adapters import XMLAdapter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9710a1c7",
   "metadata": {},
   "source": [
    "3. üåê DSPy LLM Configuration (LongCat Chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227d4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get(\"LongCat_API_KEY\")\n",
    "\n",
    "dspy.settings.configure(\n",
    "    lm=dspy.LM(\n",
    "        model=\"openai/LongCat-Flash-Chat\",\n",
    "        api_key=api_key,\n",
    "        api_base=\"https://api.longcat.chat/openai/v1\",\n",
    "        task=\"text-generation\"\n",
    "    ),\n",
    "    adapter=XMLAdapter()\n",
    ")\n",
    "\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f8abd",
   "metadata": {},
   "source": [
    "4. üåê Scrape Text from 10 URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Sustainable_agriculture\",\n",
    "    \"https://www.nature.com/articles/d41586-025-03353-5\",\n",
    "    \"https://www.sciencedirect.com/science/article/pii/S1043661820315152\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10457221/\",\n",
    "    \"https://www.fao.org/3/y4671e/y4671e06.htm\",\n",
    "    \"https://www.medscape.com/viewarticle/time-reconsider-tramadol-chronic-pain-2025a1000ria\",\n",
    "    \"https://www.sciencedirect.com/science/article/pii/S0378378220307088\",\n",
    "    \"https://www.frontiersin.org/news/2025/09/01/rectangle-telescope-finding-habitable-planets\",\n",
    "    \"https://www.medscape.com/viewarticle/second-dose-boosts-shingles-protection-adults-aged-65-years-2025a1000ro7\",\n",
    "    \"https://www.theguardian.com/global-development/2025/oct/13/astro-ambassadors-stargazers-himalayas-hanle-ladakh-india\"\n",
    "]\n",
    "\n",
    "def scrape_text(url):\n",
    "    try:\n",
    "        res = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        text = ' '.join([p.text for p in paragraphs])\n",
    "        return text[:5000]\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "data = [{'link': url, 'text': scrape_text(url)} for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033f4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158839fc",
   "metadata": {},
   "source": [
    "5. üß† Entity Extraction using DSPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b079a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityWithAttr(BaseModel):\n",
    "    entity: str = Field(description=\"The named entity\")\n",
    "    attr_type: str = Field(description=\"The semantic type (e.g., Drug, Concept, Person)\")\n",
    "\n",
    "class ExtractEntities(dspy.Signature):\n",
    "    paragraph: str = dspy.InputField()\n",
    "    entities: List[EntityWithAttr] = dspy.OutputField()\n",
    "\n",
    "extractor = dspy.Predict(ExtractEntities)\n",
    "\n",
    "# Extract entities\n",
    "for d in data:\n",
    "    try:\n",
    "        output = extractor(paragraph=d['text'])\n",
    "        d['entities'] = [e for e in output.entities if len(e.entity.strip()) <= 40 and len(e.attr_type.strip()) <= 40]\n",
    "    except Exception as e:\n",
    "        print(f\"Entity extraction failed: {e}\")\n",
    "        d['entities'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd153638",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['entities']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b6e1af",
   "metadata": {},
   "source": [
    "6. üßπ Deduplication with Confidence Feedback Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeduplicateEntities(dspy.Signature):\n",
    "    items: List[EntityWithAttr] = dspy.InputField()\n",
    "    deduplicated: List[EntityWithAttr] = dspy.OutputField()\n",
    "    confidence: float = dspy.OutputField()\n",
    "\n",
    "dedup_predictor = dspy.ChainOfThought(DeduplicateEntities)\n",
    "\n",
    "def deduplicate_with_lm(items: List[EntityWithAttr], batch_size=10, target_confidence=0.91):\n",
    "    if not items:\n",
    "        return []\n",
    "\n",
    "    def _process_batch(batch):\n",
    "        while True:\n",
    "            pred = dedup_predictor(items=batch)\n",
    "            confidence = pred.confidence or 0.0\n",
    "            if confidence >= target_confidence:\n",
    "                return pred.deduplicated\n",
    "\n",
    "    results = []\n",
    "    for i in range(0, len(items), batch_size):\n",
    "        batch = items[i:i+batch_size]\n",
    "        results.extend(_process_batch(batch))\n",
    "    return results\n",
    "\n",
    "for d in data:\n",
    "    try:\n",
    "        d['deduplicated_entities'] = deduplicate_with_lm(d['entities'])\n",
    "    except Exception as e:\n",
    "        print(f\"Deduplicated Entity extraction failed: {e}\")\n",
    "        d['deduplicated_entities'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98430b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['deduplicated_entities']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db78a0",
   "metadata": {},
   "source": [
    "7. üîó Relation Extraction + Mermaid Diagram Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relation(BaseModel):\n",
    "    subj: str = Field(description=\"subject\")\n",
    "    pred: str = Field(description=\"predicate\")\n",
    "    obj: str = Field(description=\"object\")\n",
    "\n",
    "class ExtractRelations(dspy.Signature):\n",
    "    paragraph: str = dspy.InputField()\n",
    "    entities: List[str] = dspy.InputField()\n",
    "    relations: List[Relation] = dspy.OutputField()\n",
    "    confidence: float = dspy.OutputField()\n",
    "\n",
    "rel_predictor = dspy.ChainOfThought(ExtractRelations)\n",
    "\n",
    "def triples_to_mermaid(triples: List[Relation], entity_list: List[str], max_label_len=40):\n",
    "    entity_set = {e.lower().strip() for e in entity_list}\n",
    "    lines = [\"flowchart LR\"]\n",
    "\n",
    "    def _id(s): return s.strip().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"_\")\n",
    "\n",
    "    for t in triples:\n",
    "        if t.subj.lower() in entity_set or t.obj.lower() in entity_set:\n",
    "            label = t.pred.strip()\n",
    "            if len(label) > max_label_len:\n",
    "                label = label[:max_label_len - 3] + \"...\"\n",
    "            lines.append(f'    {_id(t.subj)}[\"{t.subj}\"] -->|{label}| {_id(t.obj)}[\"{t.obj}\"]')\n",
    "\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c30e95",
   "metadata": {},
   "source": [
    "Generate Mermaid Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"mermaid\", exist_ok=True)\n",
    "\n",
    "for d in data:\n",
    "    entity_list = [e.entity for e in d['deduplicated_entities']]\n",
    "    if not entity_list:\n",
    "        continue\n",
    "    while True:\n",
    "        pred = rel_predictor(paragraph=d['text'], entities=entity_list)\n",
    "        confidence = pred.confidence or 0.0\n",
    "        if confidence >= 0.91:\n",
    "            mermaid = triples_to_mermaid(pred.relations, entity_list)\n",
    "            with open(f\"mermaid/mermaid_{data.index(d)+1}.md\", \"w\") as f:\n",
    "                f.write(mermaid)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265c873",
   "metadata": {},
   "source": [
    "8. üìä CSV Export of Deduplicated Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30465e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"CSV\", exist_ok=True)\n",
    "rows = []\n",
    "\n",
    "for d in data:\n",
    "    for e in d['deduplicated_entities']:\n",
    "        rows.append({'link': d['link'], 'tag': e.entity, 'tag_type': e.attr_type})\n",
    "\n",
    "pd.DataFrame(rows).to_csv(\"CSV/Output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8cb064",
   "metadata": {},
   "source": [
    "9. üßæ Summary & Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b797f78",
   "metadata": {},
   "source": [
    "‚úÖ What This Notebook Does:\n",
    "\n",
    "- Scrapes 10 web pages using BeautifulSoup\n",
    "- Uses DSPy + Pydantic to extract structured entity data\n",
    "- Deduplicates extracted entities using custom logic\n",
    "- Creates Mermaid diagram output for visual relationship mapping\n",
    "- Saves all extracted entities to a `Output.csv` file\n",
    "\n",
    "---\n",
    "\n",
    "üìå Notes:\n",
    "\n",
    "- LLM entity extraction is not 100% deterministic ‚Äî your output might vary run-to-run.\n",
    "- You can add confidence scoring or filter based on entity types if needed.\n",
    "- Mermaid diagrams can be extended to show richer relationships (e.g., weights, categories).\n",
    "\n",
    "---\n",
    "\n",
    "üí¨ Improvements (Optional):\n",
    "\n",
    "- Add a confidence threshold for filtering LLM output\n",
    "- Build a Streamlit app to visualize results\n",
    "- Use fuzzy string matching for smarter deduplication\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
